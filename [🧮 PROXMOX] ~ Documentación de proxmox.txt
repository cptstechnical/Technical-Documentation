=======[‚ùå TEORIA ]=================================================================================
[üßÆ Fundamento t√©cnico vCPU]:
- Overcommit: El overcommit de CPU es posible porque Proxmox/KVM permite asignar m√°s vCPUs que cores f√≠sicos.
- Linux KVM/QEMU y el scheduler del host manejan la distribuci√≥n del tiempo de CPU entre las VMs, gestionando el acceso concurrente.
- Cores asignados en Proxmox: pertenece a vCPUs (virtual CPU) que configuras para cada VM (m√°quina virtual).

[üßÆ Fundamento t√©cnico Cluster]:
- Quorum: En Proxmox VE (que utiliza Corosync como capa de clustering), el ‚Äúquorum‚Äù es el mecanismo que asegura que s√≥lo un subconjunto mayoritario de nodos pueda tomar decisiones v√°lidas sobre el estado del cl√∫ster y sus recursos. 

[üßÆ Arquitectura Interna Proxmox]:
- KVM (Kernel-based Virtual Machine):
- LXC (Linux Containers):
- Storage (ZFS, LVM, Ceph, ext4, etc): 
  + Espacio asignado por disco virtual a cada VM/CT.
  + Snapshots, backups y thin provisioning seg√∫n el tipo de almacenamiento.
- Gesti√≥n de recursos:
  + CPU: Puedes asignar n√∫cleos virtuales (vCPU). Proxmox permite overcommit.
  + RAM: Limitada, puedes usar ballooning para ajustar din√°micamente.
  + Disco: Asignado como almacenamiento virtual (raw, qcow2, LVM, ZVOL).
  + Red: Interfaces virtuales puenteadas al host (bridge).


[üßÆ Forma t√©cnica y directa seg√∫n c√≥mo Proxmox gestiona RAM y CPU]:

üß† RAM ‚Äì Repartici√≥n f√≠sica (sin overcommit):
- La RAM se reserva de forma exclusiva para cada VM/CT.
- Si tienes 4GB RAM f√≠sicas y restas ~1GB para el sistema (Proxmox + servicios), te quedan 3GB √∫tiles. Si cada VM tiebe 1GB de RAM puedo crear hasta 3 VMs.
üß© No se recomienda consumir el 100% de la RAM disponible. Siempre deja margen para el sistema (~1GB m√≠nimo en tu caso).
+--------------------+----------------------+------------------------+
| RAM disponible     | RAM por VM/CT        | M√°quinas posibles      |
+--------------------+----------------------+------------------------+
| 4GB (total)        | 1GB sistema host      | ~3GB √∫tiles           |
| ~3GB para VMs/CTs  | 1GB por VM            | Hasta 3 VMs           |
| ~3GB para VMs/CTs  | 512MB por VM          | Hasta 6 VMs           |
| ~3GB para VMs/CTs  | 256MB por CT (LXC)    | Hasta 10‚Äì11 CTs       |
+--------------------+----------------------+------------------------+

‚öôÔ∏è CPU ‚Äì Asignaci√≥n l√≥gica (overcommit permitido):
- Las vCPU son virtuales, puedes tener m√°s vCPUs que cores f√≠sicos.
- Generalmente se usa overcommit: por ejemplo, 4 cores f√≠sicos pueden soportar 6‚Äì8 vCPUs o m√°s.
- Lo importante es la carga real de uso.
+----------------+-----------------------------------+------------------------+
| Cores f√≠sicos  | VMs ligeras posibles (con 1 vCPU) | Recomendaci√≥n          |
+----------------+-----------------------------------+------------------------+
| 4              | 6‚Äì8 VMs (light use)               | OK con bajo uso        |
| 4              | 3‚Äì4 VMs (alta carga)              | Mejor rendimiento      |
+----------------+-----------------------------------+------------------------+


-------------------------------------------------------------------------------------
[üßÆ Ejemplo con Hardware (1TB HDD, 4GB RAM, 4 CPU)]:
Supuestos:
- Sistema base Proxmox requiere ~512MB ‚Äì 1GB RAM y ~10-20GB de disco.
- Asignamos 3GB RAM y ~950GB para VMs/CTs.
- CPU puede sobreprovisionarse (ej: 6-8 vCPUs repartidas en 4 cores f√≠sicos).

+----------------+---------------+------------+----------------+--------------------+
| Tipo           | RAM Asignada  | CPU (vCPU) | Disco Asignado | M√°quinas posibles  |
+----------------+---------------+------------+----------------+--------------------+
| VM ligera      | 512MB         | 1 vCPU     | 10GB           | 5‚Äì6 VMs            |
| LXC b√°sico     | 256MB         | 0.5 vCPU   | 2GB            | 8‚Äì10 CTs           |
| VM media       | 1GB           | 1 vCPU     | 20GB           | 2‚Äì3 VMs            |
| Mezcla VM+CT   | VMs: 2√ó1GB    | 2√ó1 vCPU   | 40GB total     | + 4 CTs con 512MB  |
+----------------+---------------+------------+----------------+--------------------+

üß† Recomendaciones:
Usa LXC para servicios Linux siempre que puedas ‚Üí mayor densidad.
Activa RAM ballooning en VMs si el SO lo soporta.
Usa ZFS si necesitas snapshots/flexibilidad, pero requiere m√°s RAM.
Mant√©n siempre RAM libre para el sistema (~1GB m√≠nimo).
Si vas a tener muchas VMs, considera ampliar la RAM del ejemplo.
-------------------------------------------------------------------------------------


=======[‚ùå CREAR M√ÅQUINA VIRTUAL ]==================================================================
# buena pr√°ctica para mejor visualizaci√≥n
‚öôÔ∏è > Sort Key: Name

# creo una nueva m√°quina virtual
sobre el Node indicado > Create VM

# para crear una m√°quina virtual o contenedor es recomendable hacerlo desde:
local-lvm

# Ajustes al crear la m√°quina:
Create VM
------------------------------------------------------------------------------
+ General:
*********************************************************
Name: ~
_
+ OS:
*********************************************************
Storage: Synology (para producci√≥n) | local (para pruebas o desarrollo)
OS: (selecciono la √∫ltima versi√≥n de cada S.O)
_
+ System:
*********************************************************
_
+ Disk:
*********************************************************
Bus/Devicec: SATA | 0
Storage: Synology (para producci√≥n) | local (para pruebas o desarrollo)
Disk Size: 40 GiB (o los necesarios)
Cache: Write Back
[x] Avanced
~
[x] SSD emulation
_
+ CPU:
*********************************************************
# es recomendable poner un socket y varios cores
Sockets: 1 
Cores: 4
_
+ Memory:
*********************************************************
Memory (MiB): 2048 (o los que sean necesarios, en este caso 2GB | 4096 = 4GB) 
_
+ Network:
*********************************************************
Bridge: vmbr0
VLAN Tag: (el tag de vlan correspondiente no la vlan de red)
Model: 
------------------------------------------------------------------------------





===============================================================================
                                ASIGNACI√ìN DE CPU EN PROXMOX
===============================================================================
# Cores asignados en Proxmox: vCPUs que configuras para cada VM
-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

M√©todo             | ¬øCPU f√≠sica directa?         | Descripci√≥n t√©cnica
-------------------|------------------------------|-------------------------------------------------------------
Asignar vCPU       | ‚ùå No                       | Lo m√°s com√∫n. El hipervisor distribuye el uso entre vCPUs.
CPU pinning        | ‚úÖ Parcialmente             | Asocia vCPUs a cores f√≠sicos espec√≠ficos. Ejemplos: taskset, numactl, configuraci√≥n en XML/QEMU args.
CPU passthrough    | ‚úÖ S√≠, pero para PCI        | Passthrough para hardware PCI (GPU, etc.). No aplica para CPU.


-------------------------------------------------------------------------------
Recomendaci√≥n para un host con 4 cores f√≠sicos:
+---------------------------------+---------------------------------+
| Tipo de uso                     | N¬∫ de VMs seguras (1 vCPU c/u)  |
+---------------------------------+---------------------------------+
| Carga baja (CLI, routers, logs) | 8 ‚Äì 10                          |
| Carga mixta                     | 5 ‚Äì 6                           |
| Carga alta (pentesting, GVM)    | 2 ‚Äì 3 m√°ximo simult√°neas        |
+---------------------------------+---------------------------------+

-------------------------------------------------------------------------------
¬øC√≥mo se ejecutan m√°s vCPUs que cores f√≠sicos?

- Cada vCPU es una porci√≥n de tiempo (time slice) de un core f√≠sico.
- El scheduler del hipervisor intercala tareas en milisegundos.
- Si varias VMs no usan CPU intensivamente al mismo tiempo, funciona sin problemas.
- Si todas realizan carga pesada simult√°neamente, se genera cola y latencia.

-------------------------------------------------------------------------------
Resumen clave:

‚úÖ Puedes asignar tantas vCPUs como quieras.  
‚ö†Ô∏è Est√°s repartiendo tiempo de CPU, no creando potencia adicional real.  
üß† Usa overcommit con prudencia: eval√∫a la carga, no solo el n√∫mero de vCPUs.

-------------------------------------------------------------------------------
Consejos t√©cnicos:

- Usa drivers **virtio** para disco y red, mejora rendimiento y compatibilidad.
- Habilita **ballooning** para RAM en laboratorios para mejor gesti√≥n din√°mica.
- Para menos overhead, considera usar contenedores (LXC) en vez de VMs.
- Aplica **CPU limits y CPU shares** para controlar recursos si varias VMs se activan a la vez.

-------------------------------------------------------------------------------
Fundamento t√©cnico:

- El overcommit de CPU es posible porque Proxmox/KVM permite asignar m√°s vCPUs que cores f√≠sicos.
- Linux KVM/QEMU y el scheduler del host manejan la distribuci√≥n del tiempo de CPU entre las VMs, gestionando el acceso concurrente.

===============================================================================
