# Proxmox VE (PVE) ¬∑ Documentaci√≥n 10/07/2025
=======[‚ùå TEORIA ]=================================================================================
[üßÆ Arquitectura Interna Proxmox]:
# KVM sirve para virtualizar m√°quinas completas en Linux, permiti√©ndote ejecutar m√∫ltiples sistemas operativos (VMs) aislados sobre un mismo host f√≠sico, Proxmox utiliza el KVM que ya viene en el kernel de Linux, integr√°ndolo con:
#     - QEMU (para emulaci√≥n de hardware)
#     - LXC (para contenedores)
#     - libvirt y sus propias APIs para gesti√≥n
#     - Su propia interfaz web y CLI (pve)

+-------------------+  +-------------+      
|  Virtual Machine  |  |  CONTAINER  |      # KVM (Kernel-based Virtual Machine): Es capaz de crear m√°quinas virtuales utilizando al KVM[QUEMU] del Linux principal (Proxmox)
|        KVM        |  |     LXC     |      # LXC (Linux Containers): Es capaz de crear contenedores utilizando al KVM[LXC] del Linux principal (Proxmox)
+-------------------+  +-------------+
          |                    |
+------------------------------------+
|              PROXMOX VE            |
+------------------------------------+
                  |
+------------------------------------+
|                KVM                 |
+------------------------------------+
                  |
+------------------------------------+
|               LINUX                |      # Distribuci√≥n por defecto de Proxmox: Debian
+------------------------------------+
                  |
+------------------------------------+      # Storage (ZFS, LVM, Ceph, ext4, etc)
|              HARDWARE              |      #   + Espacio asignado por disco virtual a cada VM/CT.
+------------------------------------+      #   + Snapshots, backups y thin provisioning seg√∫n el tipo de almacenamiento.
+---------------+  +-----------------+      # Gesti√≥n de recursos
|    STORAGE    |  |     NETWORK     |      #   + CPU: Puedes asignar n√∫cleos virtuales (vCPU). Proxmox permite overcommit.
+---------------+  +-----------------+      #   + RAM: Limitada, puedes usar ballooning para ajustar din√°micamente.
                                            #   + Disco: Asignado como almacenamiento virtual (raw, qcow2, LVM, ZVOL).
                                            #   + Red: Interfaces virtuales puenteadas al host (bridge).                      

[üßÆ Fundamento t√©cnico vCPU]:
- Overcommit: Overcommit (o sobre-asignaci√≥n) se refiere a la pr√°ctica de reservar m√°s recursos virtuales (CPU, memoria, disco, etc.) de los que f√≠sicamente hay disponibles en el host. El overcommit de CPU es posible porque Proxmox/KVM permite asignar m√°s vCPUs que cores f√≠sicos.
- Time-slicing: El scheduler (programador de procesos) reparte el tiempo de cada core f√≠sico en peque√±as rebanadas (‚Äútime slices‚Äù). Cada vCPU de cada VM entra en cola y recibe su rebanada cuando le toque turno, hasta que se agoten o pase el slice al siguiente hilo.
- Linux KVM/QEMU y el scheduler (programador de procesos) del host manejan la distribuci√≥n del tiempo de CPU entre las VMs, gestionando el acceso concurrente.
- Cores asignados en Proxmox: pertenece a vCPUs (virtual CPU) que configuras para cada VM (m√°quina virtual), "esto quiere decir que no pertenece a los cores f√≠sicos reales si no a los virtuales".

[üßÆ Fundamento t√©cnico Cluster] HA (pol√≠ticas de alta disponibilidad):
# Un cluster es cuando tienes 2 o m√°s nodos Proxmox trabajando juntos, compartiendo gesti√≥n de recursos, VMs, etc.

- Nodo: va a ser cada uno de los espacios dentro del ‚ÄúDatacenter‚Äù y reprensenta cada hardware (los servidores)

- split-brain: Es cuando un cluster se divide por un fallo de red, y dos partes creen que son la v√°lidas, actuando al mismo tiempo. Esto puede provocar: Corrupci√≥n de discos compartidos, Dos VMs corriendo con el mismo ID (una por nodo) e Inconsistencias.

- Quorum: En Proxmox VE (que utiliza Corosync como capa de clustering), el ‚Äúquorum‚Äù es el mecanismo que asegura que s√≥lo un subconjunto mayoritario de nodos pueda tomar decisiones v√°lidas (como iniciar/migrar/apagar) sobre el estado del cl√∫ster y sus recursos. El quorum es un mecanismo de seguridad que garantiza que el cluster tiene suficientes nodos comunic√°ndose entre s√≠ para operar con seguridad (sin perdida de datos). Sirve para evitar particiones cerebrales o split-brain. El quorum por simplificarlo sirve para saber que nodo esta activo o desactivado, funciona por votos y los dispositivos tienen que ser de n√∫mero impar. El quorum no va a permitir habilitar nuevos servicios o nuevas m√°quinas etc si el quorum esta desabilitado, en el caso de que el quorum este habilitado todo seguir√° funcionando correctamente. Como el comando "pvecm" epertenece al cluster est√° directamente relacionado con el quorum.
    Se basa en el n√∫mero de nodos activos y visibles entre s√≠:
      quorum = (n√∫mero de nodos / 2) + 1
    * Ejemplo Cluster de 3 nodos: Quorum m√≠nimo: 2 nodos activos y conectados
    * Ejemplo Cluster de 4 nodos: Quorum m√≠nimo: 3 nodos activos
    * Ejemplo Si no hay quorum: el nodo bloquea operaciones de gesti√≥n (crear/mover VMs, etc.)
    ‚ùó¬øPor qu√© mejor n√∫mero impar en quorum?
    Con n√∫mero impar: Siempre hay una mayor√≠a clara y Menos probabilidad de empates
    Con n√∫mero par: Si la red se parte 2 y 2 ‚Üí ning√∫n grupo tiene mayor√≠a y Resultado: todo se bloquea

- QDevice: Se instala en una m√°quina f√≠sica (con bajos recursos ya que solo corre un servicio llamado corosync-qnetd) cuando los nodos actuales son pares (ya que para que funcione el quorum tienen que ser impares). Es un tercer votante neutral de voto (solo vota), que no ejecuta VMs, pero sirve para desempatar y ayudar al quorum (se a√±ade a un cluster Proxmox con nodos pares para ayudar a mantener quorum).
    Es √∫til cuando: Tienes solo 2 nodos f√≠sicos y No puedes a√±adir un tercer nodo completo
    Es ligero, solo vota en el cluster y Se configura con corosync + qnetd
    * Ejemplo Tienes 2 nodos (node1 y node2): Si node1 y node2 pierden conexi√≥n entre ellos ‚Üí El qdevice vota por el que siga conectado a √©l y Solo ese nodo mantiene quorum, el otro se a√≠sla.


[üßÆ Forma t√©cnica y directa seg√∫n c√≥mo Proxmox gestiona RAM y CPU]:

üß† RAM ‚Äì Repartici√≥n f√≠sica (sin overcommit):
- La RAM se reserva de forma exclusiva para cada VM/CT.
- Si tienes 4GB RAM f√≠sicas y restas ~1GB para el sistema (Proxmox + servicios), te quedan 3GB √∫tiles. Si cada VM tiebe 1GB de RAM puedo crear hasta 3 VMs.
üß© No se recomienda consumir el 100% de la RAM disponible. Siempre deja margen para el sistema (~1GB m√≠nimo en tu caso).
+--------------------+----------------------+------------------------+
| RAM disponible     | RAM por VM/CT        | M√°quinas posibles      |
+--------------------+----------------------+------------------------+
| 4GB (total)        | 1GB sistema host      | ~3GB √∫tiles           |
| ~3GB para VMs/CTs  | 1GB por VM            | Hasta 3 VMs           |
| ~3GB para VMs/CTs  | 512MB por VM          | Hasta 6 VMs           |
| ~3GB para VMs/CTs  | 256MB por CT (LXC)    | Hasta 10‚Äì11 CTs       |
+--------------------+----------------------+------------------------+

‚öôÔ∏è CPU ‚Äì Asignaci√≥n l√≥gica (overcommit permitido):
- Las vCPU son virtuales, puedes tener m√°s vCPUs que cores f√≠sicos.
- Generalmente se usa overcommit: por ejemplo, 4 cores f√≠sicos pueden soportar 6‚Äì8 vCPUs o m√°s.
- Lo importante es la carga real de uso.
‚ö†Ô∏è Estoy repartiendo tiempo de CPU, no creando potencia adicional real.  
üß† Overcommit con prudencia: evalaci√≥n seg√∫n la carga real que cada VM va a generar y los picos de ejecuci√≥n en tiempos asignados, no solo el n√∫mero de vCPUs.
+----------------+-----------------------------------+------------------------+
| Cores f√≠sicos  | VMs ligeras posibles (con 1 vCPU) | Recomendaci√≥n          |
+----------------+-----------------------------------+------------------------+
| 4              | 6‚Äì8 VMs (light use)               | OK con bajo uso        |
| 4              | 3‚Äì4 VMs (alta carga)              | Mejor rendimiento      |
+----------------+-----------------------------------+------------------------+

M√©todo                         | ¬øCPU f√≠sica directa?         | Descripci√≥n t√©cnica
-------------------------------|------------------------------|-------------------------------------------------------------
Asignar vCPU (por defecto )    | ‚ùå No                       | Lo m√°s com√∫n. El hipervisor distribuye el uso entre vCPUs.
CPU pinning                    | ‚úÖ Parcialmente             | Asocia vCPUs a cores f√≠sicos espec√≠ficos. Ejemplos: taskset, numactl, configuraci√≥n en XML/QEMU args.
CPU passthrough                | ‚úÖ S√≠, pero para PCI        | Passthrough para hardware PCI (GPU, etc.). No aplica para CPU.

-------------------------------------------------------------------------------------
Consejos t√©cnicos:
- Usa drivers **virtio** para disco y red, mejora rendimiento y compatibilidad.
- Habilita **ballooning** para RAM en laboratorios para mejor gesti√≥n din√°mica.
- Para menos overhead, considera usar contenedores (LXC) en vez de VMs.
- Aplica **CPU limits y CPU shares** para controlar recursos si varias VMs se activan a la vez.
-------------------------------------------------------------------------------------

-------------------------------------------------------------------------------------
‚ùó Ejemplo con Hardware (1TB HDD, 4GB RAM, 4 CPU):
Supuestos:
- Sistema base Proxmox requiere ~512MB ‚Äì 1GB RAM y ~10-20GB de disco.
- Asignamos 3GB RAM y ~950GB para VMs/CTs.
- CPU puede sobreprovisionarse (ej: 6-8 vCPUs repartidas en 4 cores f√≠sicos).

+----------------+---------------+------------+----------------+--------------------+
| Tipo           | RAM Asignada  | CPU (vCPU) | Disco Asignado | M√°quinas posibles  |
+----------------+---------------+------------+----------------+--------------------+
| VM ligera      | 512MB         | 1 vCPU     | 10GB           | 5‚Äì6 VMs            |
| LXC b√°sico     | 256MB         | 0.5 vCPU   | 2GB            | 8‚Äì10 CTs           |
| VM media       | 1GB           | 1 vCPU     | 20GB           | 2‚Äì3 VMs            |
| Mezcla VM+CT   | VMs: 2√ó1GB    | 2√ó1 vCPU   | 40GB total     | + 4 CTs con 512MB  |
+----------------+---------------+------------+----------------+--------------------+

üß† Recomendaciones:
Usa LXC para servicios Linux siempre que puedas ‚Üí mayor densidad.
Activa RAM ballooning en VMs si el SO lo soporta.
Usa ZFS si necesitas snapshots/flexibilidad, pero requiere m√°s RAM.
Mant√©n siempre RAM libre para el sistema (~1GB m√≠nimo).
Si vas a tener muchas VMs, considera ampliar la RAM del ejemplo.
-------------------------------------------------------------------------------------


=======[‚ùå PASOS A SEGUIR DE INSTALACI√ìN ]==========================================================
# sigo los pasos seg√∫n la instalaci√≥n requerida:

          - Crear unidad ISO: Instalaci√≥n de ISO Proxmox VE
          - Configuraci√≥n de red:
          - Configuraci√≥n de discos y unidades: (unidad ISOs)
          - Configuraci√≥n de scripts: Scripts de la comunidad (Post, hardening...)
          - Crear cluster : En el caso de ser necesario HA ‚Üí configuro y creo cluster
                    + Configurar nodo como cluster (NTP activo y en hora)
                    + Configurar Recursos Comunes del Cluster


=======[‚ùå CREAR UNIDAD ISO ]=======================================================================
# descargar iso
https://www.proxmox.com/en/downloads

## acceso proxmox instalaci√≥n
Proxmox Virtual Environment : https://www.proxmox.com/en/downloads [Proxmox VE 8.4 ISO Installer]
- Disco en ext4 (el espacio es para la instalaci√≥n de proxmox)
- zona horaria
- <password-root>
- ip
htts://<ip>:8006


=======[‚ùå CONFIGURACI√ìN DE RED ]===================================================================
=======[‚ùå CONFIGURACI√ìN DE DISCOS Y UNIDADES ]=====================================================
=======[‚ùå CONFIGURACI√ìN DE SCRIPTS ]===============================================================
# cambio el tema
usuario (arriba a la derecha) > Color Theme > Proxmox Dark

# instalamos el script inicializador
https://community-scripts.github.io/ProxmoxVE/scripts?id=post-pve-install
(proxmox:selecciono el nodo) > Shell > (copio y pego el script)
- y /all

# instalo script para cambiar a powersave (elige como el CPU escala su frecuencia y consumo de los watts)
https://community-scripts.github.io/ProxmoxVE/scripts?id=scaling-governor
[*] powersave
- y

# instalo script para ver si el procesador tiene alguna actualizaci√≥n
https://community-scripts.github.io/ProxmoxVE/scripts?id=microcode
[*] intel-microcode_3.20240514.1_amd64.deb
- y

# ‚ö†Ô∏è de este √∫ltimo script no se si fiarme ya que es de alguien independiente y no oficial de proxmox, lo quiero dejar documentado por si ag√∫n d√≠a es √∫til para m√≠
# instalo script proxmenux para tareas de administrador de servidor (drivers, m√°quinas virtuales...)
# https://github.com/MacRimi/ProxMenux
# en la pagina princial, donde muestra el readme solo tengo que copiar y pegar el instalador : bash -c "$(wget -qLO - https://raw.githubusercontent.com/MacRimi/ProxMenux/main/install_proxmenux.sh)"
# - normal
# - y
#
# para abrirlo "menu"
# ahora desde proxmox puedo isntalar paquetes de proxmox sin tener que salir de proxmox, ahora en este ejemplo voy a instalar adguards sin salir de proxmox
# > menu
#   > Proxmox Helpers Script
#   > search : adguards
#   > yes
#   > Desafult Settings

# reinicio el servidor proxmox
- reiniciar

=======[‚ùå CREAR CLUSTER ]==========================================================================
# antes de iniciar con los cluster hay que intentar que los dispositivos o servidores sean los m√°s parecido posible a ser posible exactamente iguales

[üßÆ creo un cluster]::
# Para migrar un nodo, el nodo que vamos a migrar como cluster no debe tener ninguna m√°quina virtual ni contenedor y adem√°s que contenga el mismo NTP "timedatectl status"
# una vez tenga el nodo limpio me dirijo al otro nodo (al que vamos a migrar el nuevo nodo) y hago lo siguiente:
# ejemplo proxmox pve1
-----------------------------------------------------------------
Datacenter > Cluster (pve1) > Create Cluster
          Cluster Demo: <nombre>
          Cluster Network: <asigno_la_ip_dentro_del_cluster>
          > create
# una vez creado
Datacenter > Cluster (pve1) > Join Information > Copy Information
-----------------------------------------------------------------

# ejemplo proxmox pve2
-----------------------------------------------------------------
Datacenter > Cluster (pve2) > Join Information >           
          > Information: <pego_informaci√≥n_copiada_anteriormente>
          > Password: <contrase√±a>
-----------------------------------------------------------------

# ejemplo proxmox pve1
# refresco p√°gina y ya deber√≠a ver el cluster creado son dos nodos pve1 y pve2
# para comprobar su funcionamiento y en caso opcional puedo migrar una m√°quina del pve1 al pve2 
Datacenter > Cluster (pve1) > VM (ejemplo: 101) > Sumary > Migrate       
          > Target Mode: pve2
          > Migrate

[üßÆ configuro quorum]::
https://www.youtube.com/watch?v=t5yvfnFvQrU
# en el caso de que tenga un n√∫mero impar de dispositivos me aseguro que el quorum funciona correctamente
Datacenter > HA > Quoruom:OK


=======[‚ùå CREAR M√ÅQUINA VIRTUAL ]==================================================================
# buena pr√°ctica para mejor visualizaci√≥n
‚öôÔ∏è > Sort Key: Name

# creo una nueva m√°quina virtual
sobre el Node indicado > Create VM

# para crear una m√°quina virtual o contenedor es recomendable hacerlo desde:
local-lvm

# Ajustes al crear la m√°quina:
Create VM
------------------------------------------------------------------------------
+ General:
*********************************************************
Name: ~
_
+ OS:
*********************************************************
Storage: Synology (para producci√≥n) | local (para pruebas o desarrollo)
OS: (selecciono la √∫ltima versi√≥n de cada S.O)
_
+ System:
*********************************************************
_
+ Disk:
*********************************************************
Bus/Devicec: SATA | 0
Storage: Synology (para producci√≥n) | local (para pruebas o desarrollo)
Disk Size: 40 GiB (o los necesarios)
Cache: Write Back
         + Write back: mejor rendimiento, ideal para entornos donde la velocidad de I/O es prioritaria y hay respaldo el√©ctrico (UPS, almacenamiento con cach√© protegida). Riesgo de p√©rdida de datos en fallo.
         + Write through: mayor seguridad y consistencia, mejor para datos cr√≠ticos o producci√≥n donde la integridad es prioritaria, aunque con menor rendimiento.
  
[x] Avanced
~
[x] SSD emulation
_
+ CPU:
*********************************************************
# es recomendable poner un socket y varios cores
Sockets: 1 
Cores: 4
_
+ Memory:
*********************************************************
Memory (MiB): 2048 (o los que sean necesarios, en este caso 2GB | 4096 = 4GB) 
_
+ Network:
*********************************************************
Bridge: vmbr0
VLAN Tag: (el tag de vlan correspondiente no la vlan de red)
Model: 
------------------------------------------------------------------------------


