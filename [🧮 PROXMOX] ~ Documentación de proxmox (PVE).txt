# Proxmox VE (PVE) ¬∑ Documentaci√≥n 10/07/2025
=======[‚ùå TEORIA ]=================================================================================
[üßÆ Arquitectura Interna Proxmox]:
# KVM sirve para virtualizar m√°quinas completas en Linux, permiti√©ndote ejecutar m√∫ltiples sistemas operativos (VMs) aislados sobre un mismo host f√≠sico, Proxmox utiliza el KVM que ya viene en el kernel de Linux, integr√°ndolo con:
#     - QEMU (para emulaci√≥n de hardware)
#     - LXC (para contenedores)
#     - libvirt y sus propias APIs para gesti√≥n
#     - Su propia interfaz web y CLI (pve)

+-------------------+  +-------------+      
|  virtual Machine  |  |  CONTAINER  |      # KVM (Kernel-based Virtual Machine): Es capaz de crear m√°quinas virtuales utilizando al KVM[QUEMU] del Linux principal (Proxmox)
|        KVM        |  |     LXC     |      # LXC (Linux Containers): Es capaz de crear contenedores utilizando al KVM[LXC] del Linux principal (Proxmox)
+-------------------+  +-------------+
          |                    |
+------------------------------------+
|              PROXMOX VE            |
+------------------------------------+
                  |
+------------------------------------+
|                KVM                 |
+------------------------------------+
                  |
+------------------------------------+
|               LINUX                |      # Distribuci√≥n por defecto de Proxmox: Debian
+------------------------------------+
                  |
+------------------------------------+      # Storage (ZFS, LVM, Ceph, ext4, etc)
|              HARDWARE              |      #   + Espacio asignado por disco virtual a cada VM/CT.
+------------------------------------+      #   + Snapshots, backups y thin provisioning seg√∫n el tipo de almacenamiento.
+---------------+  +-----------------+      # Gesti√≥n de recursos
|    STORAGE    |  |     NETWORK     |      #   + CPU: Puedes asignar n√∫cleos virtuales (vCPU). Proxmox permite overcommit.
+---------------+  +-----------------+      #   + RAM: Limitada, puedes usar ballooning para ajustar din√°micamente.
                                            #   + Disco: Asignado como almacenamiento virtual (raw, qcow2, LVM, ZVOL).
                                            #   + Red: Interfaces virtuales puenteadas al host (bridge).                      

[üßÆ Fundamento t√©cnico vCPU]:
- Overcommit: Overcommit (o sobre-asignaci√≥n) se refiere a la pr√°ctica de reservar m√°s recursos virtuales (CPU, memoria, disco, etc.) de los que f√≠sicamente hay disponibles en el host. El overcommit de CPU es posible porque Proxmox/KVM permite asignar m√°s vCPUs que cores f√≠sicos.
- Time-slicing: El scheduler (programador de procesos) reparte el tiempo de cada core f√≠sico en peque√±as rebanadas (‚Äútime slices‚Äù). Cada vCPU de cada VM entra en cola y recibe su rebanada cuando le toque turno, hasta que se agoten o pase el slice al siguiente hilo.
- Linux KVM/QEMU y el scheduler (programador de procesos) del host manejan la distribuci√≥n del tiempo de CPU entre las VMs, gestionando el acceso concurrente.
- Cores asignados en Proxmox: pertenece a vCPUs (virtual CPU) que configuras para cada VM (m√°quina virtual), "esto quiere decir que no pertenece a los cores f√≠sicos reales si no a los virtuales".

[üßÆ Fundamento t√©cnico Cluster] HA (pol√≠ticas de alta disponibilidad):
# Un cluster es cuando tienes 2 o m√°s nodos Proxmox trabajando juntos, compartiendo gesti√≥n de recursos, VMs, etc.

- Nodo: va a ser cada uno de los espacios dentro del ‚ÄúDatacenter‚Äù y reprensenta cada hardware (los servidores)

- split-brain: Es cuando un cluster se divide por un fallo de red, y dos partes creen que son la v√°lidas, actuando al mismo tiempo. Esto puede provocar: Corrupci√≥n de discos compartidos, Dos VMs corriendo con el mismo ID (una por nodo) e Inconsistencias.

- Quorum: En Proxmox VE (que utiliza Corosync como capa de clustering), el ‚Äúquorum‚Äù es el mecanismo que asegura que s√≥lo un subconjunto mayoritario de nodos pueda tomar decisiones v√°lidas (como iniciar/migrar/apagar) sobre el estado del cl√∫ster y sus recursos. El quorum es un mecanismo de seguridad que garantiza que el cluster tiene suficientes nodos comunic√°ndose entre s√≠ para operar con seguridad (sin perdida de datos). Sirve para evitar particiones cerebrales o split-brain.
    Se basa en el n√∫mero de nodos activos y visibles entre s√≠:
      quorum = (n√∫mero de nodos / 2) + 1
    * Ejemplo Cluster de 3 nodos: Quorum m√≠nimo: 2 nodos activos y conectados
    * Ejemplo Cluster de 4 nodos: Quorum m√≠nimo: 3 nodos activos
    * Ejemplo Si no hay quorum: el nodo bloquea operaciones de gesti√≥n (crear/mover VMs, etc.)
    ‚ùó¬øPor qu√© mejor n√∫mero impar en quorum?
    Con n√∫mero impar: Siempre hay una mayor√≠a clara y Menos probabilidad de empates
    Con n√∫mero par: Si la red se parte 2 y 2 ‚Üí ning√∫n grupo tiene mayor√≠a y Resultado: todo se bloquea

- QDevice: Se instala en una m√°quina f√≠sica (con bajos recursos ya que solo corre un servicio llamado corosync-qnetd) cuando los nodos actuales son pares (ya que para que funcione el quorum tienen que ser impares). Es un tercer votante neutral de voto (solo vota), que no ejecuta VMs, pero sirve para desempatar y ayudar al quorum (se a√±ade a un cluster Proxmox con nodos pares para ayudar a mantener quorum).
    Es √∫til cuando: Tienes solo 2 nodos f√≠sicos y No puedes a√±adir un tercer nodo completo
    Es ligero, solo vota en el cluster y Se configura con corosync + qnetd
    * Ejemplo Tienes 2 nodos (node1 y node2): Si node1 y node2 pierden conexi√≥n entre ellos ‚Üí El qdevice vota por el que siga conectado a √©l y Solo ese nodo mantiene quorum, el otro se a√≠sla.


[üßÆ Forma t√©cnica y directa seg√∫n c√≥mo Proxmox gestiona RAM y CPU]:

üß† RAM ‚Äì Repartici√≥n f√≠sica (sin overcommit):
- La RAM se reserva de forma exclusiva para cada VM/CT.
- Si tienes 4GB RAM f√≠sicas y restas ~1GB para el sistema (Proxmox + servicios), te quedan 3GB √∫tiles. Si cada VM tiebe 1GB de RAM puedo crear hasta 3 VMs.
üß© No se recomienda consumir el 100% de la RAM disponible. Siempre deja margen para el sistema (~1GB m√≠nimo en tu caso).
+--------------------+----------------------+------------------------+
| RAM disponible     | RAM por VM/CT        | M√°quinas posibles      |
+--------------------+----------------------+------------------------+
| 4GB (total)        | 1GB sistema host      | ~3GB √∫tiles           |
| ~3GB para VMs/CTs  | 1GB por VM            | Hasta 3 VMs           |
| ~3GB para VMs/CTs  | 512MB por VM          | Hasta 6 VMs           |
| ~3GB para VMs/CTs  | 256MB por CT (LXC)    | Hasta 10‚Äì11 CTs       |
+--------------------+----------------------+------------------------+

‚öôÔ∏è CPU ‚Äì Asignaci√≥n l√≥gica (overcommit permitido):
- Las vCPU son virtuales, puedes tener m√°s vCPUs que cores f√≠sicos.
- Generalmente se usa overcommit: por ejemplo, 4 cores f√≠sicos pueden soportar 6‚Äì8 vCPUs o m√°s.
- Lo importante es la carga real de uso.
+----------------+-----------------------------------+------------------------+
| Cores f√≠sicos  | VMs ligeras posibles (con 1 vCPU) | Recomendaci√≥n          |
+----------------+-----------------------------------+------------------------+
| 4              | 6‚Äì8 VMs (light use)               | OK con bajo uso        |
| 4              | 3‚Äì4 VMs (alta carga)              | Mejor rendimiento      |
+----------------+-----------------------------------+------------------------+


-------------------------------------------------------------------------------------
[üßÆ Ejemplo con Hardware (1TB HDD, 4GB RAM, 4 CPU)]:
Supuestos:
- Sistema base Proxmox requiere ~512MB ‚Äì 1GB RAM y ~10-20GB de disco.
- Asignamos 3GB RAM y ~950GB para VMs/CTs.
- CPU puede sobreprovisionarse (ej: 6-8 vCPUs repartidas en 4 cores f√≠sicos).

+----------------+---------------+------------+----------------+--------------------+
| Tipo           | RAM Asignada  | CPU (vCPU) | Disco Asignado | M√°quinas posibles  |
+----------------+---------------+------------+----------------+--------------------+
| VM ligera      | 512MB         | 1 vCPU     | 10GB           | 5‚Äì6 VMs            |
| LXC b√°sico     | 256MB         | 0.5 vCPU   | 2GB            | 8‚Äì10 CTs           |
| VM media       | 1GB           | 1 vCPU     | 20GB           | 2‚Äì3 VMs            |
| Mezcla VM+CT   | VMs: 2√ó1GB    | 2√ó1 vCPU   | 40GB total     | + 4 CTs con 512MB  |
+----------------+---------------+------------+----------------+--------------------+

üß† Recomendaciones:
Usa LXC para servicios Linux siempre que puedas ‚Üí mayor densidad.
Activa RAM ballooning en VMs si el SO lo soporta.
Usa ZFS si necesitas snapshots/flexibilidad, pero requiere m√°s RAM.
Mant√©n siempre RAM libre para el sistema (~1GB m√≠nimo).
Si vas a tener muchas VMs, considera ampliar la RAM del ejemplo.
-------------------------------------------------------------------------------------


=======[‚ùå CREAR M√ÅQUINA VIRTUAL ]==================================================================
# buena pr√°ctica para mejor visualizaci√≥n
‚öôÔ∏è > Sort Key: Name

# creo una nueva m√°quina virtual
sobre el Node indicado > Create VM

# para crear una m√°quina virtual o contenedor es recomendable hacerlo desde:
local-lvm

# Ajustes al crear la m√°quina:
Create VM
------------------------------------------------------------------------------
+ General:
*********************************************************
Name: ~
_
+ OS:
*********************************************************
Storage: Synology (para producci√≥n) | local (para pruebas o desarrollo)
OS: (selecciono la √∫ltima versi√≥n de cada S.O)
_
+ System:
*********************************************************
_
+ Disk:
*********************************************************
Bus/Devicec: SATA | 0
Storage: Synology (para producci√≥n) | local (para pruebas o desarrollo)
Disk Size: 40 GiB (o los necesarios)
Cache: Write Back
         + Write back: mejor rendimiento, ideal para entornos donde la velocidad de I/O es prioritaria y hay respaldo el√©ctrico (UPS, almacenamiento con cach√© protegida). Riesgo de p√©rdida de datos en fallo.
         + Write through: mayor seguridad y consistencia, mejor para datos cr√≠ticos o producci√≥n donde la integridad es prioritaria, aunque con menor rendimiento.
  
[x] Avanced
~
[x] SSD emulation
_
+ CPU:
*********************************************************
# es recomendable poner un socket y varios cores
Sockets: 1 
Cores: 4
_
+ Memory:
*********************************************************
Memory (MiB): 2048 (o los que sean necesarios, en este caso 2GB | 4096 = 4GB) 
_
+ Network:
*********************************************************
Bridge: vmbr0
VLAN Tag: (el tag de vlan correspondiente no la vlan de red)
Model: 
------------------------------------------------------------------------------





===============================================================================
                                ASIGNACI√ìN DE CPU EN PROXMOX
===============================================================================
# Cores asignados en Proxmox: vCPUs que configuras para cada VM
-------------------------------------------------------------------------------

-------------------------------------------------------------------------------

M√©todo             | ¬øCPU f√≠sica directa?         | Descripci√≥n t√©cnica
-------------------|------------------------------|-------------------------------------------------------------
Asignar vCPU       | ‚ùå No                       | Lo m√°s com√∫n. El hipervisor distribuye el uso entre vCPUs.
CPU pinning        | ‚úÖ Parcialmente             | Asocia vCPUs a cores f√≠sicos espec√≠ficos. Ejemplos: taskset, numactl, configuraci√≥n en XML/QEMU args.
CPU passthrough    | ‚úÖ S√≠, pero para PCI        | Passthrough para hardware PCI (GPU, etc.). No aplica para CPU.


-------------------------------------------------------------------------------
Recomendaci√≥n para un host con 4 cores f√≠sicos:
+---------------------------------+---------------------------------+
| Tipo de uso                     | N¬∫ de VMs seguras (1 vCPU c/u)  |
+---------------------------------+---------------------------------+
| Carga baja (CLI, routers, logs) | 8 ‚Äì 10                          |
| Carga mixta                     | 5 ‚Äì 6                           |
| Carga alta (pentesting, GVM)    | 2 ‚Äì 3 m√°ximo simult√°neas        |
+---------------------------------+---------------------------------+

-------------------------------------------------------------------------------
¬øC√≥mo se ejecutan m√°s vCPUs que cores f√≠sicos?

- Cada vCPU es una porci√≥n de tiempo (time slice) de un core f√≠sico.
- El scheduler del hipervisor intercala tareas en milisegundos.
- Si varias VMs no usan CPU intensivamente al mismo tiempo, funciona sin problemas.
- Si todas realizan carga pesada simult√°neamente, se genera cola y latencia.

-------------------------------------------------------------------------------
Resumen clave:

‚úÖ Puedes asignar tantas vCPUs como quieras.  
‚ö†Ô∏è Est√°s repartiendo tiempo de CPU, no creando potencia adicional real.  
üß† Usa overcommit con prudencia: eval√∫a la carga, no solo el n√∫mero de vCPUs.

-------------------------------------------------------------------------------
Consejos t√©cnicos:

- Usa drivers **virtio** para disco y red, mejora rendimiento y compatibilidad.
- Habilita **ballooning** para RAM en laboratorios para mejor gesti√≥n din√°mica.
- Para menos overhead, considera usar contenedores (LXC) en vez de VMs.
- Aplica **CPU limits y CPU shares** para controlar recursos si varias VMs se activan a la vez.

-------------------------------------------------------------------------------
Fundamento t√©cnico:

- El overcommit de CPU es posible porque Proxmox/KVM permite asignar m√°s vCPUs que cores f√≠sicos.
- Linux KVM/QEMU y el scheduler del host manejan la distribuci√≥n del tiempo de CPU entre las VMs, gestionando el acceso concurrente.

===============================================================================
